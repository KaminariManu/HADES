{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e37e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Define class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8cdcf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])# Define transforms: resize, convert to tensor, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c751e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def unnormalize(img_tensor):\n",
    "    device = img_tensor.device\n",
    "    if img_tensor.dim() == 4:\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)\n",
    "    else:\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225], device=device).view(3, 1, 1)\n",
    "    return img_tensor * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59ffaff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import GoogLeNet_Weights\n",
    "\n",
    "# Load GoogleLeNet with the recommended weights argument\n",
    "model = models.googlenet(weights=GoogLeNet_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Change the final fully connected layer for 10 classes (CIFAR-10)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d635674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(('conv1', 'conv2', 'inception3a', 'inception3b')):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c017ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Augmentation for training\n",
    "robust_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random crop and resize\n",
    "    transforms.RandomHorizontalFlip(),                    # Random horizontal flip\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jitter\n",
    "    transforms.RandomRotation(15),                        # Random rotation\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # Random blur\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e618c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dataset for original images\n",
    "class CIFAR10TorchDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx][0]\n",
    "        img = img.astype('uint8')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Flatten y_train for stratification\n",
    "y_train_flat = y_train.flatten()\n",
    "\n",
    "# Split train into train/val (80/20) with stratification\n",
    "x_train_split, x_val, y_train_split, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=42, stratify=y_train_flat\n",
    ")\n",
    "\n",
    "# --- Augmentation setup ---\n",
    "robust_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Datasets: both normal and robust for train, only normal for val/test\n",
    "train_dataset_normal = CIFAR10TorchDataset(x_train_split, y_train_split, transform=transform)\n",
    "train_dataset_robust = CIFAR10TorchDataset(x_train_split, y_train_split, transform=robust_transform)\n",
    "from torch.utils.data import ConcatDataset\n",
    "train_dataset = ConcatDataset([train_dataset_normal, train_dataset_robust])\n",
    "\n",
    "val_dataset = CIFAR10TorchDataset(x_val, y_val, transform=transform)\n",
    "test_dataset = CIFAR10TorchDataset(x_test, y_test, transform=transform)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d4ecbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_loader, val_loader, device, epochs=5, lr=1e-3, patience=5):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "        for images, labels in loop:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).squeeze()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        loop_val = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loop_val:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device).squeeze()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "                loop_val.set_postfix(loss=loss.item())\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "       # Path per salvare i pesi del modello\n",
    "        best_model_path = \"best_model_googleLenet_aug.pth\"\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)  # Save best model to disk\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                model.load_state_dict(torch.load(best_model_path))  # Restore best model from disk\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d7744e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Should be True\n",
    "print(torch.cuda.device_count())  # Should be > 0\n",
    "print(torch.cuda.get_device_name(0))  # Should return GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85fdd007",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04cf23bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 0.4030, Train Acc: 0.8778 | Val Loss: 0.1714, Val Acc: 0.9441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 | Train Loss: 0.1351, Train Acc: 0.9566 | Val Loss: 0.1458, Val Acc: 0.9518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 | Train Loss: 0.0868, Train Acc: 0.9721 | Val Loss: 0.1484, Val Acc: 0.9528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 | Train Loss: 0.0660, Train Acc: 0.9786 | Val Loss: 0.1460, Val Acc: 0.9542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 | Train Loss: 0.0541, Train Acc: 0.9826 | Val Loss: 0.1485, Val Acc: 0.9527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 | Train Loss: 0.0464, Train Acc: 0.9849 | Val Loss: 0.1514, Val Acc: 0.9539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 | Train Loss: 0.0386, Train Acc: 0.9875 | Val Loss: 0.1576, Val Acc: 0.9555\n",
      "Early stopping at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manua\\AppData\\Local\\Temp\\ipykernel_6824\\2965275938.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))  # Restore best model from disk\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {device}\")\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-4\n",
    "\n",
    "train(model, train_loader, val_loader, device, epochs=num_epochs, lr=learning_rate, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ed6c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_normalization(imgs):\n",
    "    \"\"\"\n",
    "    Convert a batch of images from normalization (mean=0.5, std=0.5)\n",
    "    to GoogLeNet normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).\n",
    "    imgs: torch.Tensor of shape (B, 3, H, W)\n",
    "    Returns: torch.Tensor of same shape, normalized for GoogLeNet\n",
    "    \"\"\"\n",
    "    # Unnormalize from (0.5, 0.5, 0.5) to [0, 1]\n",
    "    imgs = imgs * 0.5 + 0.5\n",
    "    # Normalize to GoogLeNet\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=imgs.device).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=imgs.device).view(1, 3, 1, 1)\n",
    "    imgs = (imgs - mean) / std\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27db4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_googlenet(pretrained=True):\n",
    "    if pretrained:\n",
    "        model = models.googlenet(weights=GoogLeNet_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        model = models.googlenet(weights=None, init_weights=True)  # Explicit init\n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7cbdd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 95.40%\n"
     ]
    }
   ],
   "source": [
    "model_path = \"best_model_googleLenet_aug.pth\"\n",
    "model = get_googlenet(pretrained=True)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "model.eval()\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct_test / total_test\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e24ff507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "def run_attacks_metrics(model, test_loader, device, epsilons=[0.01, 0.03, 0.05]):\n",
    "    import foolbox as fb\n",
    "\n",
    "    model.eval()\n",
    "    fmodel = fb.PyTorchModel(model, bounds=(-1, 1))\n",
    "    class_names = ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    results = []\n",
    "\n",
    "    # For aggregate confusion\n",
    "    fgsm_agg_conf = np.zeros((10, 10), dtype=int)\n",
    "    pgd_agg_conf = np.zeros((10, 10), dtype=int)\n",
    "\n",
    "    # Collect all test images and labels\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    for images, labels in test_loader:\n",
    "        all_images.append(images)\n",
    "        all_labels.append(labels)\n",
    "    all_images = torch.cat(all_images, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    total_images = all_images.shape[0]\n",
    "\n",
    "    fgsm_conf_matrices = []\n",
    "    pgd_conf_matrices = []\n",
    "\n",
    "    # PSNR metrics\n",
    "    fgsm_psnr_per_eps = {}\n",
    "    fgsm_psnr_per_class = {}\n",
    "    pgd_psnr_per_eps = {}\n",
    "    pgd_psnr_per_class = {}\n",
    "\n",
    "    for eps in epsilons:\n",
    "        batch_size = 128\n",
    "        clean_correct = 0\n",
    "        fgsm_correct = 0\n",
    "        pgd_correct = 0\n",
    "        total = 0\n",
    "\n",
    "        fgsm_confusion = np.zeros((10, 10), dtype=int)\n",
    "        pgd_confusion = np.zeros((10, 10), dtype=int)\n",
    "\n",
    "        # For PSNR\n",
    "        fgsm_psnr_list = []\n",
    "        fgsm_psnr_per_class_list = [[] for _ in range(10)]\n",
    "        pgd_psnr_list = []\n",
    "        pgd_psnr_per_class_list = [[] for _ in range(10)]\n",
    "\n",
    "        for i in range(0, total_images, batch_size):\n",
    "            batch_imgs = all_images[i:i+batch_size].to(device)\n",
    "            batch_lbls = all_labels[i:i+batch_size].to(device)\n",
    "\n",
    "            attack_fgsm = fb.attacks.FGSM()\n",
    "            advs_fgsm, _, _ = attack_fgsm(fmodel, batch_imgs, batch_lbls, epsilons=eps)\n",
    "            attack_pgd = fb.attacks.LinfPGD(steps=10, rel_stepsize=0.1)\n",
    "            advs_pgd, _, _ = attack_pgd(fmodel, batch_imgs, batch_lbls, epsilons=eps)\n",
    "\n",
    "            batch_imgs_norm = convert_normalization(batch_imgs)  # Convert images for model\n",
    "            advs_fgsm_norm = convert_normalization(advs_fgsm)  # Convert FGSM adversarial images\n",
    "            advs_pgd_norm = convert_normalization(advs_pgd)  # Convert PGD adversarial images\n",
    "\n",
    "            clean_pred = model(batch_imgs_norm).argmax(axis=1)\n",
    "            fgsm_pred = model(advs_fgsm_norm).argmax(axis=1)\n",
    "            pgd_pred = model(advs_pgd_norm).argmax(axis=1)\n",
    "\n",
    "            clean_correct += (clean_pred == batch_lbls).sum().item()\n",
    "            fgsm_correct += (fgsm_pred == batch_lbls).sum().item()\n",
    "            pgd_correct += (pgd_pred == batch_lbls).sum().item()\n",
    "            total += batch_lbls.size(0)\n",
    "\n",
    "            for t, p in zip(batch_lbls.cpu().numpy(), fgsm_pred.cpu().numpy()):\n",
    "                fgsm_confusion[t, p] += 1\n",
    "                fgsm_agg_conf[t, p] += 1\n",
    "            for t, p in zip(batch_lbls.cpu().numpy(), pgd_pred.cpu().numpy()):\n",
    "                pgd_confusion[t, p] += 1\n",
    "                pgd_agg_conf[t, p] += 1\n",
    "\n",
    "            # FGSM PSNR calculation (unnormalize to [0,1] for PSNR)\n",
    "            batch_imgs_unnorm = (batch_imgs_norm * torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)) + torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n",
    "            advs_fgsm_unnorm = (advs_fgsm_norm * torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)) + torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n",
    "            batch_imgs_unnorm = torch.clamp(batch_imgs_unnorm, 0, 1)\n",
    "            advs_fgsm_unnorm = torch.clamp(advs_fgsm_unnorm, 0, 1)\n",
    "            for j in range(batch_imgs_unnorm.shape[0]):\n",
    "                psnr_fgsm = psnr(\n",
    "                    batch_imgs_unnorm[j].cpu().numpy(),\n",
    "                    advs_fgsm_unnorm[j].cpu().numpy(),\n",
    "                    data_range=1.0\n",
    "                )\n",
    "                fgsm_psnr_list.append(psnr_fgsm)\n",
    "                label = int(batch_lbls[j].item())\n",
    "                fgsm_psnr_per_class_list[label].append(psnr_fgsm)\n",
    "\n",
    "            # PGD PSNR calculation (unnormalize to [0,1] for PSNR)\n",
    "            advs_pgd_unnorm = (advs_pgd_norm * torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)) + torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n",
    "            advs_pgd_unnorm = torch.clamp(advs_pgd_unnorm, 0, 1)\n",
    "            for j in range(batch_imgs_unnorm.shape[0]):\n",
    "                psnr_pgd = psnr(\n",
    "                    batch_imgs_unnorm[j].cpu().numpy(),\n",
    "                    advs_pgd_unnorm[j].cpu().numpy(),\n",
    "                    data_range=1.0\n",
    "                )\n",
    "                pgd_psnr_list.append(psnr_pgd)\n",
    "                label = int(batch_lbls[j].item())\n",
    "                pgd_psnr_per_class_list[label].append(psnr_pgd)\n",
    "\n",
    "        clean_acc = 100 * clean_correct / total\n",
    "        fgsm_acc = 100 * fgsm_correct / total\n",
    "        pgd_acc = 100 * pgd_correct / total\n",
    "        results.append({'epsilon': eps, 'clean_acc': clean_acc, 'fgsm_acc': fgsm_acc, 'pgd_acc': pgd_acc})\n",
    "\n",
    "        # Store PSNR metrics\n",
    "        fgsm_psnr_per_eps[eps] = np.mean(fgsm_psnr_list) if fgsm_psnr_list else float('nan')\n",
    "        fgsm_psnr_per_class[eps] = [np.mean(fgsm_psnr_per_class_list[c]) if fgsm_psnr_per_class_list[c] else float('nan') for c in range(10)]\n",
    "        pgd_psnr_per_eps[eps] = np.mean(pgd_psnr_list) if pgd_psnr_list else float('nan')\n",
    "        pgd_psnr_per_class[eps] = [np.mean(pgd_psnr_per_class_list[c]) if pgd_psnr_per_class_list[c] else float('nan') for c in range(10)]\n",
    "\n",
    "        fgsm_conf_matrices.append(fgsm_confusion.copy())\n",
    "        pgd_conf_matrices.append(pgd_confusion.copy())\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return {\n",
    "        \"fgsm_conf_matrices\": fgsm_conf_matrices,\n",
    "        \"pgd_conf_matrices\": pgd_conf_matrices,\n",
    "        \"fgsm_agg_conf\": fgsm_agg_conf,\n",
    "        \"pgd_agg_conf\": pgd_agg_conf,\n",
    "        \"class_names\": class_names,\n",
    "        \"results\": df_results,\n",
    "        \"fgsm_psnr_per_eps\": fgsm_psnr_per_eps,\n",
    "        \"fgsm_psnr_per_class\": fgsm_psnr_per_class,\n",
    "        \"pgd_psnr_per_eps\": pgd_psnr_per_eps,\n",
    "        \"pgd_psnr_per_class\": pgd_psnr_per_class\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0ec042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attack_metrics(metrics, attack_type=\"pgd\", save_prefix=None):\n",
    "    \"\"\"\n",
    "    Print and optionally save aggregate confusion, per-class confusion, and accuracy table for FGSM or PGD attacks.\n",
    "    attack_type: \"fgsm\" or \"pgd\"\n",
    "    save_prefix: if provided, saves CSVs with this prefix (e.g., \"student_pgd\")\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    assert attack_type in (\"fgsm\", \"pgd\"), \"attack_type must be 'fgsm' or 'pgd'\"\n",
    "\n",
    "    agg_conf_key = f\"{attack_type}_agg_conf\"\n",
    "    psnr_per_eps_key = f\"{attack_type}_psnr_per_eps\"\n",
    "    psnr_per_class_key = f\"{attack_type}_psnr_per_class\"\n",
    "\n",
    "    print(f\"\\nAggregate {attack_type.upper()} confusion (all epsilons):\")\n",
    "    agg_conf = metrics[agg_conf_key]\n",
    "    agg_df = pd.DataFrame(agg_conf, index=metrics[\"class_names\"], columns=metrics[\"class_names\"])\n",
    "    print(agg_df)\n",
    "    if save_prefix:\n",
    "        agg_df.to_csv(f\"{save_prefix}_agg_conf_{attack_type}.csv\")\n",
    "\n",
    "    # Calculate mean PSNR per epsilon and per class\n",
    "    if psnr_per_eps_key in metrics and psnr_per_class_key in metrics:\n",
    "        mean_psnr_per_eps = metrics[psnr_per_eps_key]  # dict: epsilon -> mean psnr\n",
    "        mean_psnr_per_class = metrics[psnr_per_class_key]  # dict: epsilon -> [mean psnr per class]\n",
    "    else:\n",
    "        print(f\"Warning: {attack_type.upper()} PSNR metrics not found in metrics dict, computing for last epsilon only.\")\n",
    "        mean_psnr_per_eps = {}\n",
    "        mean_psnr_per_class = {}\n",
    "\n",
    "    # Per-class confusion summary with mean PSNR per class\n",
    "    summary = []\n",
    "    for idx, row in enumerate(agg_conf):\n",
    "        true_label = metrics[\"class_names\"][idx]\n",
    "        row_copy = row.copy()\n",
    "        row_copy[idx] = 0\n",
    "        total_confused = row_copy.sum()\n",
    "        if total_confused == 0:\n",
    "            most_confused = \"-\"\n",
    "            count = 0\n",
    "            percentage = 0.0\n",
    "        else:\n",
    "            most_confused_idx = np.argmax(row_copy)\n",
    "            most_confused = metrics[\"class_names\"][most_confused_idx]\n",
    "            count = row_copy[most_confused_idx]\n",
    "            percentage = 100.0 * count / total_confused\n",
    "        # Get mean PSNR for this class (for the last epsilon)\n",
    "        mean_psnr = None\n",
    "        if psnr_per_class_key in metrics and metrics[psnr_per_class_key]:\n",
    "            last_eps = list(metrics[psnr_per_class_key].keys())[-1]\n",
    "            mean_psnr = metrics[psnr_per_class_key][last_eps][idx]\n",
    "        summary.append({\n",
    "            \"True Label\": true_label,\n",
    "            \"Most Confused With\": most_confused,\n",
    "            \"Count\": count,\n",
    "            \"Percentage\": f\"{percentage:.2f}%\",\n",
    "            \"Mean PSNR\": f\"{mean_psnr:.2f}\" if mean_psnr is not None else \"-\"\n",
    "        })\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    print(summary_df.to_markdown(index=False))\n",
    "    if save_prefix:\n",
    "        summary_df.to_csv(f\"{save_prefix}_perclass_{attack_type}.csv\", index=False)\n",
    "\n",
    "    # Show accuracy table with mean PSNR per epsilon\n",
    "    print(f\"\\nAccuracy Table ({attack_type.upper()}):\")\n",
    "    df_results = metrics[\"results\"]\n",
    "    if psnr_per_eps_key in metrics:\n",
    "        df_results = df_results.copy()\n",
    "        col_name = f\"mean_psnr_{attack_type}\"\n",
    "        df_results[col_name] = df_results[\"epsilon\"].map(lambda eps: f\"{metrics[psnr_per_eps_key][eps]:.2f}\")\n",
    "    print(df_results.to_markdown(index=False))\n",
    "    if save_prefix:\n",
    "        df_results.to_csv(f\"{save_prefix}_accuracy_{attack_type}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16d2245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import GoogLeNet_Weights\n",
    "import foolbox as fb\n",
    "# Foolbox setup\n",
    "transform_fgsm = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "test_dataset_attack = CIFAR10TorchDataset(x_test, y_test, transform=transform_fgsm)\n",
    "test_loader_attack = DataLoader(test_dataset_attack, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95eccbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = np.arange(0.05, 0.21, 0.05)\n",
    "metrics_student = run_attacks_metrics(model, test_loader_attack, device, epsilons=epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "060cf375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregate FGSM confusion (all epsilons):\n",
      "            airplane  automobile  bird   cat  deer  dog  frog  horse  ship  \\\n",
      "airplane         369          21    83  3278    24    0    24      0   186   \n",
      "automobile        10         781     2  3000     4    0    24      0    48   \n",
      "bird              40           0   295  3461    35   14   142      4     8   \n",
      "cat                5           2    18  3824    11   29   106      1     1   \n",
      "deer               5           0    44  3485   283    8   158      7     9   \n",
      "dog                1           0    34  3667    26  176    80     12     4   \n",
      "frog               3           2    22  3343    45    8   569      1     7   \n",
      "horse             12           0    32  3479   203   17    35    212     9   \n",
      "ship              33          31    23  3145     5    6    40      1   702   \n",
      "truck             20         151     3  3208     9    3     8      3    58   \n",
      "\n",
      "            truck  \n",
      "airplane       15  \n",
      "automobile    131  \n",
      "bird            1  \n",
      "cat             3  \n",
      "deer            1  \n",
      "dog             0  \n",
      "frog            0  \n",
      "horse           1  \n",
      "ship           14  \n",
      "truck         537  \n",
      "| True Label   | Most Confused With   |   Count | Percentage   |   Mean PSNR |\n",
      "|:-------------|:---------------------|--------:|:-------------|------------:|\n",
      "| airplane     | cat                  |    3278 | 90.28%       |       20.22 |\n",
      "| automobile   | cat                  |    3000 | 93.20%       |       20.24 |\n",
      "| bird         | cat                  |    3461 | 93.41%       |       20.13 |\n",
      "| cat          | frog                 |     106 | 60.23%       |       20.18 |\n",
      "| deer         | cat                  |    3485 | 93.76%       |       20.08 |\n",
      "| dog          | cat                  |    3667 | 95.89%       |       20.15 |\n",
      "| frog         | cat                  |    3343 | 97.44%       |       20.14 |\n",
      "| horse        | cat                  |    3479 | 91.84%       |       20.16 |\n",
      "| ship         | cat                  |    3145 | 95.36%       |       20.15 |\n",
      "| truck        | cat                  |    3208 | 92.64%       |       20.23 |\n",
      "\n",
      "Accuracy Table (FGSM):\n",
      "|   epsilon |   clean_acc |   fgsm_acc |   pgd_acc |   mean_psnr_fgsm |\n",
      "|----------:|------------:|-----------:|----------:|-----------------:|\n",
      "|      0.05 |        95.4 |      42.72 |      0    |            32.1  |\n",
      "|      0.1  |        95.4 |      14.62 |      0    |            26.11 |\n",
      "|      0.15 |        95.4 |      10.12 |      0.01 |            22.63 |\n",
      "|      0.2  |        95.4 |      10.02 |      0.02 |            20.17 |\n"
     ]
    }
   ],
   "source": [
    "print_attack_metrics(metrics_student, attack_type=\"fgsm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c92685c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregate PGD confusion (all epsilons):\n",
      "            airplane  automobile  bird   cat  deer   dog  frog  horse  ship  \\\n",
      "airplane           0          46   744  2129   180    55   411      9   379   \n",
      "automobile       153           0   171  2349    36    73   269      8   152   \n",
      "bird             197           8     0  3051   161   215   333     20    11   \n",
      "cat               76          63   304     3   274  1240  1864    119    20   \n",
      "deer              62           4   607  2656     0   171   400     87    10   \n",
      "dog               45           2   506  2731   113     0   543     50     9   \n",
      "frog              31           2   347  3372   115   111     0      8    10   \n",
      "horse             34           1   191  2554   586   307   320      0     3   \n",
      "ship             240          99   307  2774    86    44   394      1     0   \n",
      "truck            107         408   197  2750    64    45   303     24   102   \n",
      "\n",
      "            truck  \n",
      "airplane       47  \n",
      "automobile    789  \n",
      "bird            4  \n",
      "cat            37  \n",
      "deer            3  \n",
      "dog             1  \n",
      "frog            4  \n",
      "horse           4  \n",
      "ship           55  \n",
      "truck           0  \n",
      "| True Label   | Most Confused With   |   Count | Percentage   |   Mean PSNR |\n",
      "|:-------------|:---------------------|--------:|:-------------|------------:|\n",
      "| airplane     | cat                  |    2129 | 53.23%       |       24.73 |\n",
      "| automobile   | cat                  |    2349 | 58.73%       |       24.76 |\n",
      "| bird         | cat                  |    3051 | 76.28%       |       24.66 |\n",
      "| cat          | frog                 |    1864 | 46.63%       |       24.71 |\n",
      "| deer         | cat                  |    2656 | 66.40%       |       24.62 |\n",
      "| dog          | cat                  |    2731 | 68.28%       |       24.68 |\n",
      "| frog         | cat                  |    3372 | 84.30%       |       24.67 |\n",
      "| horse        | cat                  |    2554 | 63.85%       |       24.68 |\n",
      "| ship         | cat                  |    2774 | 69.35%       |       24.66 |\n",
      "| truck        | cat                  |    2750 | 68.75%       |       24.75 |\n",
      "\n",
      "Accuracy Table (PGD):\n",
      "|   epsilon |   clean_acc |   fgsm_acc |   pgd_acc |   mean_psnr_pgd |\n",
      "|----------:|------------:|-----------:|----------:|----------------:|\n",
      "|      0.05 |        95.4 |      42.72 |      0    |           36.61 |\n",
      "|      0.1  |        95.4 |      14.62 |      0    |           30.64 |\n",
      "|      0.15 |        95.4 |      10.12 |      0.01 |           27.16 |\n",
      "|      0.2  |        95.4 |      10.02 |      0.02 |           24.69 |\n"
     ]
    }
   ],
   "source": [
    "print_attack_metrics(metrics_student, attack_type=\"pgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b1fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
